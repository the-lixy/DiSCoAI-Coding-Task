{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXL+G6uGuNNG8Sa+3uR6v+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-lixy/DiSCoAI-Coding-Task/blob/main/DiSCoAICodingTaskStreamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yUQ12Yn124-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install contractions\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install openai==0.28.1\n",
        "!pip install streamlit\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('GPTkey')"
      ],
      "metadata": {
        "id": "DuIPgeY5GEAr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile DiSCoAIapp.py\n",
        "import streamlit as st\n",
        "import contractions\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "import re\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "\n",
        "st.title(\"Dialogue Function Classifier\")\n",
        "st.write(\"Please upload dialogue as a .txt file: \")\n",
        "\n",
        "# file uploader widget\n",
        "uploaded_file = st.file_uploader(\"Choose a file\", type=[\"txt\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "  filename = uploaded_file.name\n",
        "\n",
        "  # punctuation removal using regex\n",
        "  def remove_punctuation(text):\n",
        "      return re.sub(r'[^\\w\\s\\?]', '', text)\n",
        "\n",
        "  # parse dialogue to separate speaker and utterance\n",
        "  rawUtterances = []\n",
        "  utterances = []\n",
        "  f = uploaded_file.read().decode(\"utf-8\")\n",
        "  lines = f.splitlines()\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "      if \":\" in line:\n",
        "          speaker, utterance = line.split(\":\", 1)\n",
        "          speaker = speaker.strip()\n",
        "          rawUtterances.append((speaker, utterance))\n",
        "          # normalisation steps - lowercase, noise removal, contraction expansion, remove punctuation\n",
        "          utterance = utterance.strip().lower()\n",
        "          utterance = contractions.fix(utterance)\n",
        "          utterance = remove_punctuation(utterance)\n",
        "          utterances.append((speaker, utterance))\n",
        "\n",
        "  # display original dialogue turns\n",
        "  st.write(\"Original Dialogue:\")\n",
        "  for i, (speaker, utterance) in enumerate(utterances):\n",
        "    st.write(speaker + \": \" + rawUtterances[i][1])\n",
        "\n",
        "  # (testing only) display normalised dialogue turns\n",
        "  #st.write(\"Normalised Dialogue:\")\n",
        "  #for i, (speaker, utterance) in enumerate(utterances):\n",
        "    #st.write(speaker + \": \" + utterances[i][1])\n",
        "\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  subjects = [\"i\", \"you\", \"we\", \"he\", \"she\", \"they\", \"us\"]\n",
        "\n",
        "  commitment_phrases = [\n",
        "      \"will\", \"shall\", \"must\", \"ought to\", \"have to\",\n",
        "      \"going to\", \"gotta\", \"intend to\", \"promise to\", \"swear to\", \"vow to\",\n",
        "      \"guarantee\", \"commit to\"\n",
        "  ]\n",
        "\n",
        "  proposal_phrases = [\n",
        "      \"should\", \"could\", \"would\", \"may\", \"might\", \"{subject} can\",\n",
        "      \"can possibly\", \"can perhaps\",\n",
        "      \"recommend\", \"suggest\", \"advise\",\n",
        "      \"consider\", \"plan to\", \"aim to\", \"hope to\",\n",
        "      \"wish to\", \"try to\",\n",
        "      \"supposed to\", \"expected to\",\n",
        "      \"would like to\", \"let us\"\n",
        "  ]\n",
        "\n",
        "  justification_phrases = [\n",
        "      \"because\", \"since\", \"due to\", \"as a result of\", \"true but\", \"yes but\",\n",
        "      \"therefore\", \"thus\", \"consequently\", \"for this reason\"\n",
        "  ]\n",
        "\n",
        "  query_phrases = [\n",
        "      \"can {subject}\", \"could {subject}\", \"would {subject}\", \"will {subject}\", \"may {subject}\",\n",
        "      \"might {subject}\", \"would {subject} mind\", \"do {subject} know\", \"can {subject}\", \"could {subject}\",\n",
        "      \"is it possible\", \"may {subject}\", \"shall {subject}\"\n",
        "  ]\n",
        "\n",
        "  deferral_phrases = [\n",
        "      \"maybe\", \"perhaps\", \"possibly\", \"might be\", \"could be\", \"not yet\",\n",
        "      \"let us wait\", \"hold off\", \"postpone\", \"delay\",\n",
        "      \"put off\", \"wait and see\", \"hold on\", \"defer\", \"for now\", \"need to think\"\n",
        "  ]\n",
        "\n",
        "  challenge_phrases = [\n",
        "      \"why do not {subject}\", \"why does not {subject}\", \"not convinced\", \"how about\", \"what if\",\n",
        "      \"are not {subject}\", \"is not {subject}\", \"is not it\", \"do not {subject} think\", \"would not it be\",\n",
        "      \"could not {subject}\", \"shouldn't {subject}\", \"is it really\", \"are you sure\", \"i am not sure\"\n",
        "  ]\n",
        "\n",
        "  #add all subjects to phrase lists\n",
        "  challenge_phrases = [\n",
        "    phrase.format(subject=s) if \"{subject}\" in phrase else phrase\n",
        "    for phrase in challenge_phrases\n",
        "    for s in subjects\n",
        "  ]\n",
        "\n",
        "  defferal_phrases = [\n",
        "    phrase.format(subject=s) if \"{subject}\" in phrase else phrase\n",
        "    for phrase in deferral_phrases\n",
        "    for s in subjects\n",
        "  ]\n",
        "\n",
        "  query_phrases = [\n",
        "    phrase.format(subject=s) if \"{subject}\" in phrase else phrase\n",
        "    for phrase in query_phrases\n",
        "    for s in subjects\n",
        "  ]\n",
        "\n",
        "  proposal_phrases = [\n",
        "    phrase.format(subject=s) if \"{subject}\" in phrase else phrase\n",
        "    for phrase in proposal_phrases\n",
        "    for s in subjects\n",
        "  ]\n",
        "\n",
        "  commitment_phrases = [\n",
        "    phrase.format(subject=s) if \"{subject}\" in phrase else phrase\n",
        "    for phrase in commitment_phrases\n",
        "    for s in subjects\n",
        "  ]\n",
        "\n",
        "  justification_phrases = [\n",
        "    phrase.format(subject=s) if \"{subject}\" in phrase else phrase\n",
        "    for phrase in justification_phrases\n",
        "    for s in subjects\n",
        "  ]\n",
        "\n",
        "\n",
        "  matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "\n",
        "  matcher.add(\"COMMITMENT\", [nlp(text) for text in commitment_phrases])\n",
        "  matcher.add(\"PROPOSAL\", [nlp(text) for text in proposal_phrases])\n",
        "  matcher.add(\"JUSTIFICATION\", [nlp(text) for text in justification_phrases])\n",
        "  matcher.add(\"QUERY\", [nlp(text) for text in query_phrases])\n",
        "  matcher.add(\"DEFERRAL\", [nlp(text) for text in deferral_phrases])\n",
        "  matcher.add(\"CHALLENGE\", [nlp(text) for text in challenge_phrases])\n",
        "\n",
        "  def categorize_most_confident_category(text):\n",
        "    doc = nlp(text)\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    if \"?\" in text:\n",
        "        return {\"category\": \"query\", \"matches\": [\"?\"]}\n",
        "\n",
        "    counts = {\n",
        "        \"commitment\": 0,\n",
        "        \"proposal\": 0,\n",
        "        \"justification\": 0,\n",
        "        \"query\": 0,\n",
        "        \"deferral\": 0,\n",
        "        \"challenge\": 0\n",
        "    }\n",
        "    matched_phrases = {\n",
        "        \"commitment\": [],\n",
        "        \"proposal\": [],\n",
        "        \"justification\": [],\n",
        "        \"query\": [],\n",
        "        \"deferral\": [],\n",
        "        \"challenge\": []\n",
        "    }\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        label = nlp.vocab.strings[match_id].lower()\n",
        "        counts[label] += 1\n",
        "        matched_phrases[label].append(span.text)\n",
        "\n",
        "    # find category with max matches\n",
        "    max_category = max(counts, key=counts.get)\n",
        "\n",
        "    # if no matches found, return \"statement\"\n",
        "    if counts[max_category] == 0:\n",
        "        return {\"category\": \"statement\", \"matches\": [\"none found\"]}\n",
        "\n",
        "    return {\"category\": max_category, \"matches\": matched_phrases[max_category]}\n",
        "\n",
        "    # classify all turns\n",
        "  classified = []\n",
        "  for _, utterance in utterances:\n",
        "      classified += [categorize_most_confident_category(utterance)]\n",
        "\n",
        "  # display a table of utterances and their classifications\n",
        "  import pandas as pd\n",
        "  results = []\n",
        "  for i, (speaker, _) in enumerate(utterances):\n",
        "      results.append({\n",
        "          \"Speaker\": speaker,\n",
        "          \"Utterance\": rawUtterances[i][1],\n",
        "          \"Function\": classified[i]['category'],\n",
        "          \"Reason\": \", \".join(f'\"{m}\"' for m in classified[i]['matches'])\n",
        "      })\n",
        "  df = pd.DataFrame(results)\n",
        "  st.table(df)  # or st.dataframe(df)\n",
        "\n",
        "\n",
        "  # format the results for passing to the LLM\n",
        "  i = 1\n",
        "  formattedResults = []\n",
        "  formattedResults.append(\"Speaker: \" + \"Utterance\" + \" | \" + \"Dialogue Function\")\n",
        "\n",
        "  for i, (speaker, utterance) in enumerate(utterances):\n",
        "    formattedResults.append(speaker + \": \" + rawUtterances[i][1] + \" | \" + classified[i]['category'] + \"\\n\")\n",
        "\n",
        "  api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "  if not api_key:\n",
        "      st.error(\"API key not found. Please set it in Colab before running the app.\")\n",
        "\n",
        "  # Task 2 - LLM\n",
        "  # make sure the API key is available\n",
        "  openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "  # request\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\":\n",
        "          \"Very briefly summarise this conversation in past-tense narrative format. You must always make reference to the supplied dialogue function (Proposal, Challenge, Commitment, Justification, Query, Deferral, or Statement) of each utterance. Do not include the actual dialogue. Do not make your own assumptions about the dialogue function of any utterances.\"\n",
        "          + str(formattedResults)}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  st.write(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH1G_DyoGIKm",
        "outputId": "41a8450d-082f-420e-e15a-227615bf4102"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting DiSCoAIapp.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import re\n",
        "\n",
        "# start streamlit app\n",
        "def run_streamlit():\n",
        "    subprocess.run([\"streamlit\", \"run\", \"DiSCoAIapp.py\"])\n",
        "\n",
        "threading.Thread(target=run_streamlit).start()\n",
        "\n",
        "# give Streamlit some time to boot\n",
        "time.sleep(5)\n",
        "\n",
        "# start localtunnel and capture output\n",
        "lt_process = subprocess.Popen(\n",
        "    ['lt', '--port', '8501', '--subdomain', 'albu-streamlit'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# read and display the public URL from localtunnel's output\n",
        "for line in lt_process.stdout:\n",
        "    if 'your url is:' in line:\n",
        "        url = re.search(r'(https://.*\\.loca\\.lt)', line)\n",
        "        if url:\n",
        "            print(\"Streamlit available at:\", url.group(1), \".\", \"Tunnel may ask you for a password, the cell below will show you the password.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "DiUXhAKgGIfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "id": "XcEjkKk0GL-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment this to kill Streamlit and LocalTunnel processes\n",
        "#!pkill streamlit\n",
        "#!pkill lt"
      ],
      "metadata": {
        "id": "QyQoBCnbHQmE"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}